{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Path management"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base: str\n",
    "if os.getcwd() == \"/kaggle/working\":\n",
    "    base = \"/kaggle\"\n",
    "else:\n",
    "    base = os.path.join(os.getcwd())\n",
    "\n",
    "def get_full_dir(sub_dir: str) -> str:\n",
    "    return os.path.join(base, sub_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sample_submission = pd.read_csv(get_full_dir('input/playground-series-s3e14/sample_submission.csv'))\n",
    "df_train = pd.read_csv(get_full_dir('input/playground-series-s3e14/train.csv'), index_col='id')\n",
    "df_test = pd.read_csv(get_full_dir('input/playground-series-s3e14/test.csv'), index_col='id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### There are no missing values in our train data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sample_submission.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(df_train.select_dtypes(include='number').columns), ncols=4, figsize=(18, 80))\n",
    "axes = axes.flatten()\n",
    "\n",
    "i = 0\n",
    "for col in df_train.select_dtypes(include='number').columns:\n",
    "    sns.kdeplot(df_train[col], label='train', ax=axes[i], fill=False)\n",
    "    sns.histplot(df_train[col], label='train', ax=axes[i + 1], stat=\"density\", bins=50)\n",
    "\n",
    "    if col != 'yield':\n",
    "        sns.kdeplot(df_test[col], label='test', ax=axes[i], fill=False)\n",
    "        sns.histplot(df_test[col], label='test', ax=axes[i + 1], stat=\"density\", bins=50)\n",
    "\n",
    "    if col != 'yield':\n",
    "        tmp_data = pd.DataFrame({\"train\": df_train[col], \"test\": df_test[col]})\n",
    "        sns.boxplot(data=tmp_data, ax=axes[i + 2])\n",
    "    else:\n",
    "        tmp_data = pd.DataFrame({\"train\": df_train[col]})\n",
    "        sns.boxplot(data=tmp_data, ax=axes[i + 2])\n",
    "    axes[i + 2].set_xlabel(col)\n",
    "\n",
    "    sns.scatterplot(x=col, y=\"yield\", label='train', ax=axes[i + 3], data=df_train)\n",
    "\n",
    "    axes[i].legend()\n",
    "    axes[i + 1].legend()\n",
    "    axes[i + 3].legend()\n",
    "    i += 4\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The apper to be some true outliers in the honeybee colum around 18 and 6, the mean for that column is ~0.389"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_honeybee_outlier_count(df: pd.DataFrame):\n",
    "    honeybee_data = df['honeybee']\n",
    "    total = honeybee_data.count()\n",
    "    below_one = honeybee_data[honeybee_data < 1].count()\n",
    "    above_17 = honeybee_data[honeybee_data > 17].count()\n",
    "    one_to_17 = honeybee_data[honeybee_data > 1].count() - above_17\n",
    "    print(f\" Total:\\t\\t {total}\")\n",
    "    print(f\" x < 1:\\t\\t {below_one}\")\n",
    "    print(f\" 1 < x < 17: {one_to_17}\")\n",
    "    print(f\" 17 < x:\\t {above_17}\")\n",
    "\n",
    "print(\"Train\")\n",
    "show_honeybee_outlier_count(df_train)\n",
    "print(\"Test\")\n",
    "show_honeybee_outlier_count(df_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Although the Train and Test datasets have a relatively small number of outliers (8 and 6, respectively), representing less than 0.1% of the datasets, it may still be beneficial to remove them. While outliers can sometimes be genuine observations, they can also have a significant impact on statistical analysis and machine learning models, leading to biased results and poor predictive performance. (Removing the outliers from the train set has improved both evaluation loss and loss on the public set upon kaggle submission)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_feature_correlation(df: pd.DataFrame, title: str):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr_matrix, dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, mask=mask)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "show_feature_correlation(df_train, \"Train\")\n",
    "show_feature_correlation(df_test, \"Test\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### MaxOfUpperTRange, MinOfUpperTRange, AverageOfUpperTRange, MaxOfLowerTRange, MinOfLowerTRange, AverageOfLowerTRange are all perfectly correlated and AverageRainingDays is almost perfectly correlated to RainingDays.\n",
    "\n",
    "##### When two or more features in a dataset are highly correlated, they can provide redundant information to the model, which does not add any additional information over the other features. This redundancy can cause instability in the model and lead to biased predictions.\n",
    "\n",
    "##### Decision tree-based algorithms, such as Random Forest, LGBM and XGBoost, choose a subset of features for consideration at each node. This situation can lead to a bias towards the correlated features, which may negatively impact the model's performance.\n",
    "\n",
    "##### To mitigate this issue we will remove all but one of the perfectly correlated features. We will remove RainingDays leaving AverageRainingDays and MaxOfUpperTRange, MinOfUpperTRange, AverageOfUpperTRange, MaxOfLowerTRange and MinOfLowerTRange leaving AverageOfLowerTRange\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare data for training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Handle outliers\n",
    "df_train = df_train[df_train['honeybee'] < 1]\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "#df_test[df_test['honeybee'] > 1] = df_train['honeybee'].mean()\n",
    "\n",
    "# Remove perfectly correlated features\n",
    "features_to_remove = ['RainingDays', 'MaxOfUpperTRange', 'MinOfUpperTRange', 'AverageOfUpperTRange', 'MaxOfLowerTRange', 'MinOfLowerTRange']\n",
    "df_train.drop(features_to_remove, axis=1)\n",
    "df_test.drop(features_to_remove, axis=1)\n",
    "\n",
    "# Scale features\n",
    "standard_scaler = StandardScaler()\n",
    "X = df_train.drop(columns=['yield'])\n",
    "X_scaled = standard_scaler.fit_transform(X)\n",
    "df_train = pd.concat([pd.DataFrame(X_scaled, columns=X.columns), df_train['yield']], axis=1)\n",
    "X_scaled = standard_scaler.transform(df_test)\n",
    "df_test = pd.DataFrame(X_scaled, columns=X.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_dist_lgbm = {\n",
    "    'n_estimators': [1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_child_weight': [1, 5, 10, 15, 20],\n",
    "    'subsample': [0.5, 0.7, 0.9],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "    'reg_alpha': [0, 0.1, 0.2, 0.3, 1],\n",
    "    'reg_lambda': [0, 0.1, 0.2, 0.3, 1],\n",
    "    'num_leaves': [12, 16, 20],\n",
    "    'objective': [\"mae\"]\n",
    "}\n",
    "\n",
    "param_dist_catboost = {\n",
    "    'iterations': [1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'depth': [4, 5, 6, 7],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'subsample': [0.5, 0.7, 0.9],\n",
    "    'colsample_bylevel': [0.5, 0.7, 0.9],\n",
    "    'random_strength': [0.1, 0.5, 1],\n",
    "    'bagging_temperature': [0.1, 0.5, 1],\n",
    "    'loss_function': ['MAE'],\n",
    "    'verbose': [False]\n",
    "}\n",
    "\n",
    "param_dist_xgboost = {\n",
    "    'n_estimators': [1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_child_weight': [1, 5, 10, 15, 20],\n",
    "    'subsample': [0.5, 0.7, 0.9],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 1],\n",
    "    'reg_alpha': [0, 0.1, 0.2, 0.3, 1],\n",
    "    'reg_lambda': [0, 0.1, 0.2, 0.3, 1],\n",
    "    'objective': [\"reg:squarederror\", 'reg:absoluteerror'],\n",
    "    'eval_metric': ['mae'],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def param_search(model, param_space, n_iter) -> RandomizedSearchCV:\n",
    "    # Load data\n",
    "    X_train = pd.read_csv(get_full_dir('input/playground-series-s3e14/train.csv'), index_col='id')\n",
    "    y_train = X_train.pop('yield')\n",
    "\n",
    "    # Create RandomizedSearchCV object\n",
    "    rs = RandomizedSearchCV(model, param_distributions=param_space, cv=5, n_iter=n_iter, scoring='neg_mean_absolute_error')\n",
    "    # Fit the model on the training data\n",
    "    rs.fit(X_train, y_train)\n",
    "    # Return results\n",
    "    return rs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Param searches the space\n",
    "search = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if search:\n",
    "    # Defines models\n",
    "    lgb_model = lgb.LGBMRegressor()\n",
    "    cat_model = CatBoostRegressor()\n",
    "    xgb_model = xgb.XGBRegressor(tree_method = 'gpu_hist')\n",
    "\n",
    "    lgb_rs = param_search(lgb_model, param_dist_lgbm, 100)\n",
    "    print(\"Best LGBM parameters:\", lgb_rs.best_params_)\n",
    "    print(\"Best LGBM MAE score:\", -lgb_rs.best_score_)\n",
    "    lgb_param = lgb_rs.best_params_\n",
    "\n",
    "    cat_rs = param_search(cat_model, param_dist_catboost, 100)\n",
    "    print(\"Best CatBoost parameters:\", cat_rs.best_params_)\n",
    "    print(\"Best CatBoost MAE score:\", -cat_rs.best_score_)\n",
    "    cat_param = cat_rs.best_params_\n",
    "\n",
    "    xgb_rs = param_search(xgb_model, param_dist_xgboost, 100)\n",
    "    print(\"Best XGBoost parameters:\", xgb_rs.best_params_)\n",
    "    print(\"Best XGBoost MAE score:\", -xgb_rs.best_score_)\n",
    "    xgb_param = xgb_rs.best_params_\n",
    "else:\n",
    "    # Best LGBM MAE score: 343.512189589491\n",
    "    lgb_param = {'subsample': 0.7, 'reg_lambda': 0.1, 'reg_alpha': 0.3, 'objective': 'mae', 'num_leaves': 20, 'n_estimators': 1000, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 0.9}\n",
    "    # Best CatBoost MAE score: 343.17189579386667\n",
    "    cat_param =  {'verbose': False, 'subsample': 0.9, 'random_strength': 0.1, 'loss_function': 'MAE', 'learning_rate': 0.05, 'l2_leaf_reg': 5, 'iterations': 1000, 'depth': 5, 'colsample_bylevel': 0.7, 'bagging_temperature': 1}\n",
    "    # Best XGBoost MAE score: 352.23548398423674\n",
    "    xgb_param = {'subsample': 0.7, 'reg_lambda': 0.1, 'reg_alpha': 0.1, 'objective': 'reg:squarederror', 'n_estimators': 1000, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.01, 'gamma': 0.2, 'eval_metric': 'mae', 'colsample_bytree': 0.9}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "\n",
    "    def __init__(self, model='XGB'):\n",
    "        self.model_type = model\n",
    "        if model == 'LGB':\n",
    "            self.model = lgb.LGBMRegressor(\n",
    "                #num_leaves = 16,\n",
    "                #max_depth=5,\n",
    "                #learning_rate = 0.01,\n",
    "                #n_estimators=1000,\n",
    "                #objective = \"mae\",\n",
    "                **lgb_param\n",
    "            )\n",
    "        elif model == 'CatBoost':\n",
    "            self.model = CatBoostRegressor(\n",
    "                #iterations=1000,\n",
    "                #learning_rate=0.01,\n",
    "                #depth=4,\n",
    "                **cat_param\n",
    "            )\n",
    "        else:\n",
    "            self.model = xgb.XGBRegressor(\n",
    "                #objective = 'reg:absoluteerror',\n",
    "                #tree_method = 'gpu_hist',\n",
    "                #colsample_bytree = 0.6,\n",
    "                #gamma = 0.8,\n",
    "                #learning_rate = 0.01,\n",
    "                #max_depth = 5,\n",
    "                #min_child_weight = 5,\n",
    "                #n_estimators = 1000,\n",
    "                #subsample = 0.7\n",
    "                **xgb_param\n",
    "            )\n",
    "\n",
    "    def fit(self, X, y, X_val, y_val):\n",
    "        self.model.fit(X, y.ravel(), eval_set=[(X_val, y_val.ravel())], verbose=False)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def grid_search(self, X, y, X_eval, y_eval):\n",
    "       pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model_type):\n",
    "    X = df_train.drop(['yield'], axis=1)\n",
    "    y = df_train['yield']\n",
    "    SKFs = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    losses = []\n",
    "    pipelines = []\n",
    "    for fold, (idx_tr, idx_vl) in enumerate(SKFs.split(X, y)):\n",
    "        train_dataframe = df_train.iloc[idx_tr]\n",
    "        dev_dataframe = df_train.iloc[idx_vl]\n",
    "\n",
    "        # splits data to features and target\n",
    "        X_train = train_dataframe.drop('yield', axis=1)\n",
    "        y_train = train_dataframe['yield']\n",
    "        X_dev = dev_dataframe.drop('yield', axis=1)\n",
    "        y_dev = dev_dataframe['yield']\n",
    "\n",
    "        # crates and fits a pipeline\n",
    "        pipelineMy = Pipeline(model=model_type)\n",
    "        pipelineMy.fit(X_train, y_train, X_dev, y_dev)\n",
    "\n",
    "        # evaluates the model\n",
    "        pipelines.append(pipelineMy)\n",
    "        loss = mean_absolute_error(y_dev, pipelineMy.predict(X_dev))\n",
    "        losses.append(loss)\n",
    "        print(f'Fold {fold} loss: {loss}')\n",
    "    print(f'Mean loss: {np.array(losses).mean()}')\n",
    "    return losses, pipelines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lossesLGB, pipelinesLGB = train('LGB')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lossesCB, pipelinesCB = train('CatBoost')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lossesXGB, pipelinesXGB = train('XGB')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Submission"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = pipelinesLGB[0].predict(df_test)\n",
    "for i in range(1, len(pipelinesLGB)):\n",
    "    preds += pipelinesLGB[i].predict(df_test)\n",
    "preds = preds / 5.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['yield'] = preds\n",
    "submission.index += 15289"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=True, header=True, index_label=\"id\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
